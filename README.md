# Deep Unsupervised Hashing via External Guidance

This repository contains the code for the paper **"Deep Unsupervised Hashing via External Guidance"** (ICML 2025).

Our approach aims to leverage external knowledge beyond the inherent visual structure of images to facilitate unsupervised hashing code learning. By introducing external textual information, we provide richer and more precise guidance for hashing network learning, thereby enhancing the discriminability of the generated binary codes.

![Model Overview](./image_1.png)
---

## ğŸ“ Project Structure
```
main/
â”œâ”€â”€ data/ # Data directory
â”‚   â”œâ”€â”€ cifar-10-batches-py/ # cifar-10 dataset
â”‚   â”‚   â””â”€â”€ train_index.txt
â”‚   â”œâ”€â”€ flickr25k/ # Flickr25k dataset
â”‚   â”‚   â”œâ”€â”€ query_index.txt
â”‚   â”‚   â”œâ”€â”€ retrieval_index.txt
â”‚   â”‚   â””â”€â”€ train_index.txt
â”‚   â”œâ”€â”€ mscoco/ # MSCOCO dataset
â”‚   â”‚   â”œâ”€â”€ database.txt
â”‚   â”‚   â”œâ”€â”€ test.txt
â”‚   â”‚   â””â”€â”€ train.txt
â”‚   â”œâ”€â”€ nus-wide/ # NUS-WIDE dataset
â”‚   â”‚   â”œâ”€â”€ database_img.txt
â”‚   â”‚   â”œâ”€â”€ database_label_onehot.txt
â”‚   â”‚   â”œâ”€â”€ test_img.txt
â”‚   â”‚   â”œâ”€â”€ test_label_onehot.txt
â”‚   â”‚   â”œâ”€â”€ train_img.txt
â”‚   â”‚   â””â”€â”€ train_label_onehot.txt
â”‚   â””â”€â”€ WordNet_Nouns.csv # Nouns from WordNet
â”œâ”€â”€ dataset.py # Dataset loader
â”œâ”€â”€ dataset_extract.py # Data extraction scripts
â”œâ”€â”€ eval_hash.py # Evaluation script
â”œâ”€â”€ models.py # Model architecture definitions
â”œâ”€â”€ nouns_embed.py # External guidance embedding script
â”œâ”€â”€ README.md # Project readme
â”œâ”€â”€ requirements.txt # Project requirements
â””â”€â”€ train.py # Main training script
```

---

## âš™ï¸ How to Run the Code

Hereâ€™s how to run this project step by step:

1ï¸âƒ£ **Extract visual features**  
First, run `dataset_extract.py` to preprocess and extract visual features from the images in each dataset:

```bash
python dataset_extract.py
```

2ï¸âƒ£ **Extract external textual features**  
Next, run nouns_embed.py to preprocess and extract external textual features (e.g., from WordNet):

```bash
python nouns_embed.py
```

3ï¸âƒ£ **Train and evaluate the hashing network**  
Finally, run train.py to train the deep hashing network and evaluate its performance:

```bash
python train.py
```
---

## ğŸ“š Datasets

Below is a list of the open-source datasets used in this paper, along with their corresponding sources.

| Dataset                           | Corresponding Sources                                               |
|-----------------------------------|---------------------------------------------------------------------|
| **CIFAR-10**                      | [Will be downloaded automatically](https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz) |
| **Flickr25k**                     | [https://press.liacs.nl/mirflickr/](https://press.liacs.nl/mirflickr/mirdownload.html) |
| **MSCOCO**                        | [Obtained from FSCH (TCSVT 2023)](https://github.com/huanglab-research/FSCH/tree/main)                |
| **NUS-WIDE**                      | [Obtained from DSDH (NeurIPS 2017)](https://github.com/Tree-Shu-Zhao/DSDH_PyTorch?tab=readme-ov-file) |
| **WordNet**                       | [https://wordnet.princeton.edu/](https://wordnet.princeton.edu/)    |
